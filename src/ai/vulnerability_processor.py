from typing import Dict, List, Any
from datetime import datetime, timedelta
import re
import aiohttp
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class VulnerabilityProcessor:
    def __init__(self):
        self.cvss_threshold = 7.0  # High severity threshold
        
    def process_nvd_data(self, data: Dict) -> List[Dict[str, Any]]:
        """Process NVD vulnerability data."""
        processed_vulns = []
        
        for vuln in data.get('vulnerabilities', []):
            cve = vuln.get('cve', {})
            
            # Get CVSS score
            metrics = cve.get('metrics', {})
            cvss_v3 = metrics.get('cvssMetricV31', [{}])[0].get('cvssData', {})
            cvss_v2 = metrics.get('cvssMetricV2', [{}])[0].get('cvssData', {})
            
            base_score = cvss_v3.get('baseScore', cvss_v2.get('baseScore', 0))
            
            # Only include high severity vulnerabilities
            if base_score >= self.cvss_threshold:
                processed_vulns.append({
                    'cve_id': cve.get('id'),
                    'description': cve.get('description', {}).get('default', ''),
                    'published': cve.get('published'),
                    'base_score': base_score,
                    'attack_vector': cvss_v3.get('attackVector', cvss_v2.get('accessVector')),
                    'affected_products': self._extract_affected_products(cve),
                    'references': [ref.get('url') for ref in cve.get('references', [])]
                })
        
        return processed_vulns
    
    def _extract_affected_products(self, cve: Dict) -> List[Dict[str, Any]]:
        """Extract affected products and versions."""
        products = []
        for config in cve.get('configurations', []):
            for node in config.get('nodes', []):
                for cpe in node.get('cpeMatch', []):
                    parts = cpe.get('criteria', '').split(':')
                    if len(parts) > 4:
                        products.append({
                            'vendor': parts[3],
                            'product': parts[4],
                            'version': parts[5] if len(parts) > 5 else '*',
                            'update': parts[6] if len(parts) > 6 else '*'
                        })
        return products 

    async def get_vulnerability_data(self) -> List[Dict[str, Any]]:
        """Fetch vulnerability data from multiple sources."""
        all_vulns = []
        
        async with aiohttp.ClientSession() as session:
            # Fetch from NVD
            nvd_vulns = await self._fetch_nvd_data(session)
            all_vulns.extend(nvd_vulns)
            
            # Fetch from GitHub Security Advisories
            github_vulns = await self._fetch_github_advisories(session)
            all_vulns.extend(github_vulns)
            
            # Fetch from Snyk Vulnerability DB
            snyk_vulns = await self._fetch_snyk_vulns(session)
            all_vulns.extend(snyk_vulns)
            
            # Fetch from ExploitDB
            exploits = await self._fetch_exploit_db(session)
            all_vulns.extend(exploits)
        
        return self._deduplicate_vulnerabilities(all_vulns)

    async def _fetch_github_advisories(self, session: aiohttp.ClientSession) -> List[Dict[str, Any]]:
        """Fetch vulnerability data from GitHub Security Advisories."""
        try:
            headers = {
                'Accept': 'application/vnd.github.v3+json',
                'Authorization': f'token {self.github_token}'
            }
            
            async with session.get(
                'https://api.github.com/graphql',
                headers=headers,
                json={
                    'query': '''
                    query {
                        securityVulnerabilities(first: 100, orderBy: {field: UPDATED_AT, direction: DESC}) {
                            nodes {
                                advisory {
                                    id
                                    summary
                                    description
                                    severity
                                    publishedAt
                                    references {
                                        url
                                    }
                                }
                                package {
                                    name
                                    ecosystem
                                }
                                vulnerableVersionRange
                            }
                        }
                    }
                    '''
                }
            ) as response:
                data = await response.json()
                return self._process_github_advisories(data)
        except Exception as e:
            return []

    async def _fetch_snyk_vulns(self, session: aiohttp.ClientSession) -> List[Dict[str, Any]]:
        """Fetch vulnerability data from Snyk Vulnerability DB."""
        try:
            headers = {
                'Authorization': f'token {self.snyk_token}'
            }
            
            async with session.get(
                'https://snyk.io/api/v1/vulns',
                headers=headers
            ) as response:
                data = await response.json()
                return self._process_snyk_vulns(data)
        except Exception as e:
            return []

    async def _fetch_exploit_db(self, session: aiohttp.ClientSession) -> List[Dict[str, Any]]:
        """Fetch exploit data from Exploit-DB."""
        try:
            async with session.get(
                'https://www.exploit-db.com/download/exploits.json'
            ) as response:
                data = await response.json()
                return self._process_exploit_db(data)
        except Exception as e:
            return []

    def _deduplicate_vulnerabilities(self, vulns: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove duplicate vulnerabilities based on ID and description similarity."""
        unique_vulns = {}
        
        for vuln in vulns:
            vuln_id = vuln.get('id') or vuln.get('cve_id')
            if vuln_id:
                if vuln_id not in unique_vulns:
                    unique_vulns[vuln_id] = vuln
                else:
                    # Merge additional information
                    existing = unique_vulns[vuln_id]
                    existing['references'] = list(set(
                        existing.get('references', []) + vuln.get('references', [])
                    ))
                    if vuln.get('exploit_available'):
                        existing['exploit_available'] = True
                
        return list(unique_vulns.values()) 

class VulnerabilityCorrelator:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(
            stop_words='english',
            ngram_range=(1, 2),
            max_features=10000
        )
        
    def correlate_vulnerabilities(self, vulns: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Correlate vulnerabilities based on description similarity and other factors."""
        if not vulns:
            return []
            
        # Extract descriptions and create TF-IDF matrix
        descriptions = [v.get('description', '') for v in vulns]
        tfidf_matrix = self.vectorizer.fit_transform(descriptions)
        
        # Calculate similarity matrix
        similarity_matrix = cosine_similarity(tfidf_matrix)
        
        # Find clusters of related vulnerabilities
        clusters = self._cluster_vulnerabilities(similarity_matrix, threshold=0.3)
        
        # Enhance vulnerabilities with correlation info
        enhanced_vulns = []
        for i, vuln in enumerate(vulns):
            cluster = next(c for c in clusters if i in c)
            related_vulns = [vulns[j] for j in cluster if j != i]
            
            # Calculate attack chain probability
            attack_chain_prob = self._calculate_attack_chain_probability(vuln, related_vulns)
            
            # Calculate severity escalation
            severity_escalation = self._calculate_severity_escalation(vuln, related_vulns)
            
            enhanced_vulns.append({
                **vuln,
                'related_vulnerabilities': [v['id'] for v in related_vulns],
                'attack_chain_probability': attack_chain_prob,
                'severity_escalation': severity_escalation,
                'correlation_factors': self._get_correlation_factors(vuln, related_vulns)
            })
            
        return enhanced_vulns
    
    def _cluster_vulnerabilities(self, similarity_matrix: np.ndarray, threshold: float) -> List[set]:
        """Cluster vulnerabilities based on similarity."""
        n_vulns = similarity_matrix.shape[0]
        clusters = []
        processed = set()
        
        for i in range(n_vulns):
            if i in processed:
                continue
                
            cluster = {i}
            for j in range(n_vulns):
                if i != j and similarity_matrix[i, j] >= threshold:
                    cluster.add(j)
                    
            clusters.append(cluster)
            processed.update(cluster)
            
        return clusters
    
    def _calculate_attack_chain_probability(self, 
                                         vuln: Dict[str, Any],
                                         related_vulns: List[Dict[str, Any]]) -> float:
        """Calculate probability of vulnerabilities being used in attack chain."""
        factors = []
        
        # Check if vulnerabilities affect same component
        if any(self._affects_same_component(vuln, rv) for rv in related_vulns):
            factors.append(0.8)
            
        # Check if vulnerabilities have similar attack vectors
        if any(self._has_similar_attack_vector(vuln, rv) for rv in related_vulns):
            factors.append(0.6)
            
        # Check temporal factors
        if any(self._is_temporally_related(vuln, rv) for rv in related_vulns):
            factors.append(0.4)
            
        return max(factors) if factors else 0.0
    
    def _calculate_severity_escalation(self,
                                    vuln: Dict[str, Any],
                                    related_vulns: List[Dict[str, Any]]) -> str:
        """Calculate potential severity escalation when vulnerabilities are combined."""
        base_severity = self._get_severity_score(vuln)
        max_combined_severity = base_severity
        
        for rv in related_vulns:
            combined_severity = self._combine_severity_scores(base_severity,
                                                           self._get_severity_score(rv))
            max_combined_severity = max(max_combined_severity, combined_severity)
            
        if max_combined_severity > base_severity + 2:
            return 'CRITICAL'
        elif max_combined_severity > base_severity:
            return 'ELEVATED'
        else:
            return 'UNCHANGED'
    
    def _get_correlation_factors(self,
                               vuln: Dict[str, Any],
                               related_vulns: List[Dict[str, Any]]) -> List[str]:
        """Get factors contributing to vulnerability correlation."""
        factors = []
        
        for rv in related_vulns:
            if self._affects_same_component(vuln, rv):
                factors.append('Same Component')
            if self._has_similar_attack_vector(vuln, rv):
                factors.append('Similar Attack Vector')
            if self._is_temporally_related(vuln, rv):
                factors.append('Temporal Relation')
            if self._shares_dependencies(vuln, rv):
                factors.append('Shared Dependencies')
                
        return list(set(factors)) 